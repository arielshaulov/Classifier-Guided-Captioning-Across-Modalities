<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Classifier-Guided Captioning Across Modalities">
  <meta name="keywords" content="Audio captioning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Classifier-Guided Captioning Across Modalities</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <style>
    body {
      font-family: 'Roboto', sans-serif;
    }
    .hero {
      background: linear-gradient(135deg, #ff7eb3, #ff758c);
      color: #fff;
    }
    .publication-title {
      font-weight: 700;
      font-size: 2rem;
    }
    .publication-authors a {
      text-decoration: underline;
      color: #fff;
    }
    .publication-links .button {
      margin: 5px;
    }
    footer {
      background-color: #f5f5f5;
      padding: 20px 0;
      font-size: 0.9rem;
    }
    footer a {
      color: #3273dc;
      text-decoration: none;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }
    table th, table td {
      border: 1px solid #ddd;
      padding: 10px;
      text-align: left;
    }
    table th {
      background-color: #f2f2f2;
      color: black;
    }
  </style>
</head>
<body>

<section class="hero is-medium">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title publication-title">Classifier-Guided Captioning Across Modalities</h1>
      <p class="subtitle publication-authors">
        <a href="https://scholar.google.com/citations?hl=en&user=A31XmP0AAAAJ">Ariel Shaulov</a>, 
        <a href="https://scholar.google.com/citations?hl=en&user=SyA5pnoAAAAJ">Tal Shaharabany</a>, 
        <a href="https://il.linkedin.com/in/eitan-shaar">Eitan Shaar</a>, 
        <a href="https://chechiklab.biu.ac.il/">Gal Chechik</a>, 
        <a href="http://www.cs.tau.ac.il/~wolf/">Lior Wolf</a>
      </p>
      <div class="publication-links">
        <a href="https://arxiv.org/abs/2501.03183" class="button is-rounded is-link">
          <span class="icon">
            <i class="ai ai-arxiv"></i>
          </span>
          <span>arXiv</span>
        </a>
        <a href="https://github.com/arielshaulov/Classifier-Guided-Captioning-Across-Modalities" class="button is-rounded is-dark">
          <span class="icon">
            <i class="fab fa-github"></i>
          </span>
          <span>Code</span>
        </a>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Abstract</h2>
    <p class="content has-text-justified">
      Most current captioning systems use language models trained on data from specific settings, such as image-based captioning via Amazon Mechanical Turk, limiting their ability to generalize to other modality distributions and contexts. This limitation hinders performance in tasks like audio or video captioning, where different semantic cues are needed. Addressing this challenge is crucial for creating more adaptable and versatile captioning frameworks applicable across diverse real-world contexts. Our framework improves captioning quality across modalities and achieves state-of-the-art results.
    </p>
  </div>
</section>

<section class="section">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Audibility Dataset</h2>
    <p class="has-text-centered">Examples from our dataset, highlighting audible and non-audible instances:</p>
    <table>
      <thead>
        <tr>
          <th>Audible</th>
          <th>Not Audible</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>The barking of a dog in excitement.</td>
          <td>Magnets attract metals.</td>
        </tr>
        <tr>
          <td>Ringing phone awaits an answer.</td>
          <td>Ice covers the lake in winter.</td>
        </tr>
        <!-- Add more rows as needed -->
      </tbody>
    </table>
  </div>
</section>

<section class="section">
  <div class="container">
    <h2 class="title is-3">BibTeX</h2>
    <p>If you find this project useful for your research, please cite:</p>
    <pre><code>@article{shaulov2025classifier,
  title={Classifier-Guided Captioning Across Modalities},
  author={Shaulov, Ariel and Shaharabany, Tal and Shaar, Eitan and Chechik, Gal and Wolf, Lior},
  journal={arXiv preprint arXiv:2501.03183},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>. Source code adapted from <a href="https://nerfies.github.io/">Nerfies</a> project.
    </p>
    <p>
      <a href="https://github.com/arielshaulov" class="icon">
        <i class="fab fa-github"></i>
      </a>
      <a href="https://github.com/talshaharabany" class="icon">
        <i class="fab fa-github"></i>
      </a>
    </p>
  </div>
</footer>

</body>
</html>
